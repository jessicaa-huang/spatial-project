This spatial vision project focuses on analyzing how well Monocular Depth Estimation Models like **MiDaS v.3.1** and **Depth-Anywhere** perform under images & videos we created under Blender and real life. We run the models on our images and videos on the depth models, and compare the model's normalized estimated depth vs. the real normalized depth value. You can take a look at some of the resulting graphs in the Jupyter Notebook located at /depth_models/depth_mean.ipynb
